{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Install necessary packages (if not already installed)\n!pip install bitsandbytes accelerate transformers sentence-transformers",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport transformers\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import pipeline",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "summarizer = pipeline(\"summarization\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load the model and tokenizer\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    quantization_config={\"load_in_4bit\": True},\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n)\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0  # Ensure pipeline uses GPU if available\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load Sentence-BERT model for semantic search\nsbert_model = SentenceTransformer('all-MiniLM-L6-v2').to('cuda')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load and preprocess the CSV dataset\ncsv_path = '/content/merged_file.csv'  # Update this with the path to your CSV file\ndf = pd.read_csv(csv_path)\ncorpus = df['Merged'].tolist()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n# Encode the corpus using Sentence-BERT\ncorpus_embeddings = sbert_model.encode(corpus, convert_to_tensor=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def retrieve_documents(query, top_n=5):\n    \"\"\"Retrieve the top_n most relevant documents from the corpus using semantic search, and summarize them.\"\"\"\n    query_embedding = sbert_model.encode(query, convert_to_tensor=True)\n    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n    top_indices = torch.topk(cos_scores, k=top_n).indices\n\n    # Retrieve and summarize documents\n    summaries = []\n    for i in top_indices:\n        document = corpus[i]\n        summary = summarizer(document)[0]['summary_text']\n        summaries.append(summary)\n    return summaries",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def chat_with_model():\n    print(\"You can start chatting now. Type 'exit' or 'quit' to end the conversation.\")\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert in all fields\"}\n    ]\n    while True:\n        user_input = input(\"User: \")\n        if user_input.lower() in [\"exit\", \"quit\"]:\n            break\n\n        # Retrieve and summarize documents\n        retrieved_summaries = retrieve_documents(user_input)\n\n        # Concatenate retrieved summaries into a single context\n        context = ' '.join(retrieved_summaries)\n\n        # Add user input to messages\n        messages.append({\"role\": \"user\", \"content\": user_input})\n\n        # Create prompt with conversation history and context\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages] + [f\"Context: {context}\"])\n\n        # Tokenize the prompt\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n\n        # Move inputs to the same device as the model\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n        # Generate response\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs.get(\"attention_mask\"),  # Pass attention mask if available\n            max_new_tokens=1000,\n            do_sample=True,\n            temperature=0.6,\n            top_p=0.9\n        )\n\n        # Decode and print the response\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"Assistant: {response}\")\n\n        # Add model response to messages\n        messages.append({\"role\": \"assistant\", \"content\": response})\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Start the chat\nchat_with_model()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}