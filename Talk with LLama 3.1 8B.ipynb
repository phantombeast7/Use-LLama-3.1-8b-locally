{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "714a5736-ab2a-404b-a58c-09edb5205ee6",
      "cell_type": "code",
      "source": "!pip install bitsandbytes accelerate --upgrade transformers",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9e764d72-3267-4565-966e-c511b6b7f0b5",
      "cell_type": "code",
      "source": "from huggingface_hub import notebook_login\nnotebook_login() ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d8a5c392-fee7-4a64-97fc-ed0e7134db36",
      "cell_type": "code",
      "source": "import transformers\nimport torch\nfrom transformers import AutoModelForCausalLM",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7bc1a0c8-a65b-417e-8bdd-929fadd352b7",
      "cell_type": "code",
      "source": "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\n        \"torch_dtype\": torch.float16,\n        \"quantization_config\": {\"load_in_4bit\": True},\n        \"low_cpu_mem_usage\": True,\n    },\n    trust_remote_code=True\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert Ai Scientist\"},\n]\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0dbebe76-346d-497d-9adb-d5b4e617d450",
      "cell_type": "code",
      "source": "while True:\n    user_input = input(\"You: \")\n    messages.append({\"role\": \"user\", \"content\": user_input})\n\n    prompt = pipeline.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    terminators = [\n        pipeline.tokenizer.eos_token_id,\n        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n    outputs = pipeline(\n        prompt,\n        max_new_tokens=200,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.6,\n        top_p=0.9,\n    )\n\n    response = outputs[0][\"generated_text\"][len(prompt):]\n    print(f\"AI: {response}\")\n\n    messages.append({\"role\": \"assistant\", \"content\": response})\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}